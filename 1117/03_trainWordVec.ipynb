{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora,models,similarities,utils\n",
    "import logging\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from keras.preprocessing import text,sequence\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取训练数据\n",
    "def getDataLen(inFile):\n",
    "    # 统计所有出现的词\n",
    "    word_ctr = collections.Counter()\n",
    "    # 评论的最大长度\n",
    "    maxlen = 0\n",
    "    len_ctr = collections.Counter()\n",
    "    \n",
    "    # 读入训练数据           \n",
    "    f=open(inFile)\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        article = line.replace('\\n','').split(\" \")\n",
    "\n",
    "        # 内容\n",
    "        content = article[1:]\n",
    "\n",
    "        # 获得评论的最大长度\n",
    "        if len(content) > maxlen:\n",
    "            maxlen = len(content)\n",
    "\n",
    "        # 统计各种长度的文章个数\n",
    "        len_ctr[str(len(content))] += 1\n",
    "\n",
    "\n",
    "    f.close()\n",
    "        \n",
    "    print('max_len ',maxlen)\n",
    "    print ('len_ctr ', len_ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLabel(x):\n",
    "    if x == '__label__NEGATIVE':\n",
    "        lable = '0'\n",
    "    elif x== '__label__POSITIVE':\n",
    "        lable = '1'\n",
    "    else:\n",
    "        print \"x=\",x\n",
    "        lable = '0'\n",
    "    return lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取训练数据\n",
    "def getTrainSet(inFile,ptype):\n",
    "    # 训练集\n",
    "    train_set=[]\n",
    "    title_set = []\n",
    "    # 读入训练数据  \n",
    "    f=open(inFile)\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        article = line.replace('\\n','').split(\" \")\n",
    "        if ptype == 'train':\n",
    "            title = getLabel(article[0])\n",
    "        elif ptype== 'test':\n",
    "            title = article[0]\n",
    "        title_set.append(title)\n",
    "        # 内容\n",
    "        train_set.append(article[1:])\n",
    "\n",
    "    f.close()\n",
    "        \n",
    "    return (title_set,train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练word2vec\n",
    "def trainModel(inFile,modelFile,vecFile):\n",
    "    # 读入数据    \n",
    "    title_set,data_set = getTrainSet(inFile,'train')\n",
    "    \n",
    "    # 训练\n",
    "    # 少于min_count次数的单词会被丢弃掉, 默认值为5\n",
    "    # size = 神经网络的隐藏层的单元数 default value is 100\n",
    "    # workers= 控制训练的并行:default = 1 worker (no parallelization) 只有在安装了Cython后才有效\n",
    "    model = models.Word2Vec(data_set,min_count=5,window=10,size = 200,workers=4)\n",
    "    \n",
    "    # 存储模型\n",
    "    model.save(modelFile)\n",
    "    \n",
    "    # 存储vector\n",
    "    model.wv.save_word2vec_format(vecFile, binary=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把原始文本转化为由词汇表索引表示的矩阵\n",
    "def fastBuildSeq(inFile,outFile,modelFile,vecFile,ptype):\n",
    "    # 读入数据\n",
    "    title_set,data_set = getTrainSet(inFile,ptype)\n",
    "    \n",
    "    # 装载模型\n",
    "    model = models.Word2Vec.load(modelFile)\n",
    "    word_vec = model.wv.load_word2vec_format(vecFile, binary=True) \n",
    "    \n",
    "    # 使用dir(object)查看对象的属性\n",
    "    # 对每一个文章做转换      \n",
    "    # 注意：由于word2vec的向量在训练的时候用的是unicode的编码，\n",
    "    # 所以在字典里面匹配key的时候，需要把key转化为unicode的编码，使用decode('utf-8')\n",
    "    transfrom = []\n",
    "    for news in data_set:\n",
    "        trs_news = [word_vec.vocab[w.decode('utf-8')].index for w in news if w.decode('utf-8') in word_vec.vocab]\n",
    "#         # --- 调试\n",
    "#         trs_news = []\n",
    "#         for w in news:\n",
    "#             if w.decode('utf-8') in word_vec.vocab:\n",
    "#                 print \"in vocab = \",w.decode('utf-8')\n",
    "#                 trs_news.append((word_vec.vocab[w.decode('utf-8')].index,w))\n",
    "#         # --\n",
    "        transfrom.append(trs_news)\n",
    "    \n",
    "#     for x in transfrom:\n",
    "#         print x\n",
    "    \n",
    "    # 对文字序列做补齐 ，补齐长度=最长的文章长度 ，补齐在最后，补齐用的词汇默认是词汇表index=0的词汇，也可通过value指定\n",
    "    # 训练好的w2v词表的index = 0 对应的词汇是空格\n",
    "    X = sequence.pad_sequences(transfrom,maxlen=300,padding='post')\n",
    "    \n",
    "    if ptype == 'train':\n",
    "        y = np.array([int(i) for i in title_set])\n",
    "        # 保存到文件\n",
    "        np.save(outFile,np.column_stack([X,y]))\n",
    "    elif ptype == 'test':\n",
    "        np.save(outFile,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data2Mat(inFile,modelFile,vecFile,partOut,totalOut,ptype):\n",
    "    \n",
    "    # 使用训练出的任意一个词向量，把全部train数据转化为向量矩阵\n",
    "    # 把分词以后的文本转化为供CNN训练的数据矩阵\n",
    "    # 由于原始数据较大，每10w分割为一个文件，分别转化\n",
    "    for (tf,po) in zip(inFile,partOut):\n",
    "        fastBuildSeq(tf,po,modelFile,vecFile,ptype)\n",
    "    \n",
    "    # 把转化完成的5个数据矩阵做合并\n",
    "    mergeNpy(partOut,totalOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeNpy(part,total):\n",
    "    # 把转化完成的5个数据矩阵做合并\n",
    "    for idx,f in enumerate(part):\n",
    "        if idx == 0:\n",
    "            tmp = np.load(f)\n",
    "            mat = tmp\n",
    "        else:\n",
    "            tmp = np.load(f)\n",
    "            mat = np.vstack([mat,tmp])\n",
    "       \n",
    "    np.save(total,mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # 定义文件路径\n",
    "    dataPath = \"/home/hadoop/DataSencise/bdci2017/BDCI2017-360/data/\"\n",
    "    mdlPath = \"/home/hadoop/DataSencise/bdci2017/BDCI2017-360/model/\"\n",
    "    \n",
    "    # 训练数据\n",
    "    inFile = [dataPath + \"train/train_m\"+ str(x) + \".txt\" for x in range(1,6)]\n",
    "    modelFile = [mdlPath + \"w2v_m\"+ str(x) + \".mdl\" for x in range(1,6)]\n",
    "    vecFile = [mdlPath + \"w2v_m\"+ str(x) + \".bin\" for x in range(1,6)]\n",
    "    trainPartMat = [dataPath + \"train/train_m\"+ str(x) + \".npy\" for x in range(1,6)]\n",
    "    trainTotalMat = dataPath + \"train/train_totalMat.npy\"\n",
    "    \n",
    "    # 测试数据\n",
    "    testFile = [dataPath + \"test/test_m\"+ str(x) + \".txt\" for x in range(1,6)]\n",
    "    # 定义输出文件名\n",
    "    testPartMat = [dataPath + \"test/test_m\"+ str(x) + \".npy\" for x in range(1,6)]\n",
    "    testTotalMat = dataPath + \"test/test_totalMat.npy\"\n",
    "    \n",
    "#     for f in testFile:\n",
    "#         getDataLen(f)\n",
    "        \n",
    "    \n",
    "    # 训练词向量模型\n",
    "    # 把原始train数据，每10w条为一组，分别训练词向量\n",
    "    # 一共训练出5个词向量模型\n",
    "#     for (tf,mf,vf) in zip(inFile,modelFile,vecFile):\n",
    "#         trainModel(tf,mf,vf)\n",
    "    \n",
    "#     # 把训练数据转成矩阵\n",
    "#     data2Mat(inFile,modelFile[0],vecFile[0],trainPartMat,trainTotalMat,'train')\n",
    "\n",
    "    # 把测试数据转成矩阵\n",
    "    data2Mat(testFile,modelFile[0],vecFile[0],testPartMat,testTotalMat,'test')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
