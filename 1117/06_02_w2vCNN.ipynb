{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "from __future__ import print_function  \n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer  \n",
    "from keras.preprocessing.sequence import pad_sequences  \n",
    "from keras.utils.np_utils import to_categorical  \n",
    "from keras.layers import Dense, Input, Flatten,Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding,Merge\n",
    "from keras.models import Model  \n",
    "from keras.optimizers import *  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Merge \n",
    "from keras.models import model_from_json\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainModel(data,modelFile,cnn_mdl,test,text):\n",
    "    # 划分数据集\n",
    "    X =data[:,:-1]\n",
    "    y =data[:,-1]\n",
    "    \n",
    "    print(\"shape of X:\",X.shape)\n",
    "    print(\"shape of y:\",y.shape)\n",
    "    \n",
    "    train_set,test_set,train_tag,test_tag = train_test_split(X,y,test_size = 0.3)\n",
    "    \n",
    "    # 把一维的标签做onehot，pd.get_dummies的结果是df,把df转为ndarray (as_matrix())\n",
    "    train_label = pd.get_dummies(train_tag).as_matrix()\n",
    "    test_label = pd.get_dummies(test_tag).as_matrix()\n",
    "    \n",
    "    print('shape of train set:',train_set.shape)\n",
    "    print('shape of train label:',train_label.shape)\n",
    "    print('shape of test set:',test_set.shape)\n",
    "    print('shape of test label:',test_label.shape)\n",
    "    \n",
    "    # 读入模型   \n",
    "    model = models.Word2Vec.load(modelFile)\n",
    "    # 读入word2vec模型提供的嵌入层，权重需要训练\n",
    "    model_embedding = model.wv.get_embedding_layer(train_embeddings=False)\n",
    "\n",
    "    # 训练嵌入层权重\n",
    "    kmodel = Sequential()\n",
    "    kmodel.add(model_embedding)\n",
    "    kmodel.compile('rmsprop', 'mse')\n",
    "    \n",
    "#     print(model_embedding.get_weights())\n",
    "    \n",
    "    # 定义嵌入层\n",
    "    # trainable=True 通过训练来更新权重\n",
    "    # trainable=Fasle 由于使用了word2vec提供的权重，这里不用再训练了\n",
    "    embedding_layer = Embedding(input_dim = model_embedding.input_dim,\n",
    "                                output_dim = 200,\n",
    "                                input_length = 300,\n",
    "                                weights = model_embedding.get_weights(),\n",
    "                                trainable = False)\n",
    "    \n",
    "    model_left = Sequential() \n",
    "    \n",
    "    # 使用word2vec训练好的嵌入层\n",
    "    model_left.add(embedding_layer)\n",
    "    \n",
    "    model_left.add(Conv1D(64, 3, padding='same',activation='relu')) \n",
    "    model_left.add(MaxPooling1D(3)) \n",
    "    model_left.add(Conv1D(64, 3, padding='same',activation='relu')) \n",
    "    model_left.add(MaxPooling1D(3)) \n",
    "    model_left.add(Conv1D(64, 3, padding='same',activation='relu')) \n",
    "    model_left.add(MaxPooling1D(18))\n",
    "    model_left.add(Flatten())\n",
    "    \n",
    "\n",
    "    model_right = Sequential() \n",
    "    \n",
    "    # 使用word2vec训练好的嵌入层\n",
    "    model_right.add(embedding_layer)\n",
    "    \n",
    "    model_right.add(Conv1D(64, 4, padding='same',activation='relu')) \n",
    "    model_right.add(MaxPooling1D(4)) \n",
    "    model_right.add(Conv1D(64, 4, padding='same',activation='relu')) \n",
    "    model_right.add(MaxPooling1D(4)) \n",
    "    model_right.add(Conv1D(64, 4, padding='same',activation='relu')) \n",
    "    model_right.add(MaxPooling1D(12))\n",
    "    model_right.add(Flatten()) \n",
    "    \n",
    "   \n",
    "    model_cent = Sequential() \n",
    "    \n",
    "    # 使用word2vec训练好的嵌入层\n",
    "    model_cent.add(embedding_layer)\n",
    "    \n",
    "    model_cent.add(Conv1D(64, 5, padding='same',activation='relu')) \n",
    "    model_cent.add(MaxPooling1D(5)) \n",
    "    model_cent.add(Conv1D(64, 5, padding='same',activation='relu')) \n",
    "    model_cent.add(MaxPooling1D(5)) \n",
    "    model_cent.add(Conv1D(64, 5, padding='same',activation='relu')) \n",
    "    model_cent.add(MaxPooling1D(8))\n",
    "    model_cent.add(Flatten()) \n",
    "    \n",
    "\n",
    "    \n",
    "#     merged = Merge([model_left, model_right,model_cent], mode='concat') \n",
    "    \n",
    "    # 最终                       \n",
    "    model_merge = Sequential() \n",
    "                   \n",
    "    model_merge.add(Merge([model_left, model_right,model_cent],mode='concat'))\n",
    "    model_merge.add(Dense(128, activation='relu')) # 全连接层  \n",
    "    model_merge.add(Dense(2, activation='softmax')) # softmax，输出文本属于20种类别中每个类别的概率  \n",
    "    \n",
    "   \n",
    "    #\n",
    "#     plot_model(model,to_file=cnn_mdl+'.png',show_shapes=True)\n",
    "    # 优化器我这里用了adadelta，也可以使用其他方法  \n",
    "    model_merge.compile(loss='categorical_crossentropy',  \n",
    "                  optimizer='Adadelta',  \n",
    "                  metrics=['accuracy'])  \n",
    "\n",
    "    # =下面开始训练，nb_epoch是迭代次数，可以高一些，训练效果会更好，但是训练会变慢  \n",
    "    model_merge.fit(train_set, train_label,epochs=1,batch_size = 1000)  \n",
    "\n",
    "    # 存储模型\n",
    "    saveMdl(model_left,cnn_mdl+\"_left\")\n",
    "    \n",
    "    # 存储模型\n",
    "    saveMdl(model_right,cnn_mdl+\"_right\")\n",
    "    \n",
    "    # 存储模型\n",
    "    saveMdl(model_cent,cnn_mdl+\"_cent\")\n",
    "    \n",
    "    # 存储模型\n",
    "    saveMdl(model_merge,cnn_mdl+\"_merge\")\n",
    "    \n",
    "    pred(model_merge,text,test,cnn_mdl[0])\n",
    "    \n",
    "#     score = model.evaluate(train_set, train_label, verbose=0) # 评估模型在训练集中的效果，准确率约99%  \n",
    "#     print('train score:', score[0])  \n",
    "#     print('train accuracy:', score[1])  \n",
    "    \n",
    "#     score = model_merge.evaluate(test_set,test_label, verbose=0)  # 评估模型在测试集中的效果，准确率约为97%，迭代次数多了，会进一步提升  \n",
    "#     print('Test score:', score[0])  \n",
    "#     print('Test accuracy:', score[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveMdl(model,mdlFile):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(mdlFile+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(mdlFile+\".h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTestSet(inFile):\n",
    "\n",
    "    # 情感标签集\n",
    "    docid_set = []\n",
    "    # 读入训练数据\n",
    "    f=open(inFile)\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        article = line.replace('\\n','').split(' ')\n",
    "        \n",
    "        # 情感标签\n",
    "        docid_set.append(article[0])\n",
    "\n",
    "    f.close()\n",
    "    return docid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeFile(outputfile,newline):\n",
    "    \n",
    "    fw = open(outputfile, 'ab')\n",
    "    fw.write(newline.encode(\"utf-8\"))\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadMdl(mdl):\n",
    "    # load json and create model\n",
    "    json_file = open(mdl+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(mdl+\".h5\")\n",
    "    print(\"Loaded \" + mdl + \" from disk\")    \n",
    "    \n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred(model,text,npy,mdl):\n",
    "\n",
    "    docid = getTestSet(text)  \n",
    "    lable = model.predict(npy)\n",
    "    output = text + \"_submit.txt\"\n",
    "    \n",
    "    for idx,lab in enumerate(lable):\n",
    "        if lab[0]>lab[1]:\n",
    "            tag = 'NEGATIVE'\n",
    "        else:\n",
    "            tag = 'POSITIVE'\n",
    "        newline = docid[idx] + \",\" + tag + \"\\n\"\n",
    "        writeFile(output,newline)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadPred(text,npy,mdl):\n",
    "\n",
    "    docid = getTestSet(text)\n",
    "    \n",
    "    model_left = loadMdl(mdl+'_left')\n",
    "    model_right = loadMdl(mdl+'_right')\n",
    "    model_cent = loadMdl(mdl+'_cent')\n",
    "    model_merge = loadMdl(mdl+'_merge')\n",
    "    \n",
    "    model_merge.compile(loss='categorical_crossentropy',optimizer='Adadelta',metrics=['accuracy']) \n",
    "    \n",
    "#     model = load_model(mdl)\n",
    "    lable = model_merge.predict(npy)\n",
    "    output = text + \"_submit.txt\"\n",
    "  \n",
    "    for idx,lab in enumerate(lable):\n",
    "        if lab[0]>lab[1]:\n",
    "            tag = 'NEGATIVE'\n",
    "        else:\n",
    "            tag = 'POSITIVE'\n",
    "        newline = d_id + \",\" + str(lable[idx]) + \"\\n\"\n",
    "        writeFile(output,newline)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # 定义文件路径\n",
    "    dataPath = \"/home/hadoop/DataSencise/bdci2017/BDCI2017-360/data/\"\n",
    "    mdlPath = \"/home/hadoop/DataSencise/bdci2017/BDCI2017-360/model/\"\n",
    "    \n",
    "    inFile = [dataPath + \"train/train_m\"+ str(x) + \".npy\" for x in range(1,6)]\n",
    "    \n",
    "    testFile = [dataPath + \"test/test_m\"+ str(x) + \".npy\" for x in range(1,6)]\n",
    "    textFile = [dataPath + \"test/test_m\"+ str(x) + \".txt\" for x in range(1,6)]\n",
    "    \n",
    "    modelFile = [mdlPath + \"w2v_m\"+ str(x) + \".mdl\" for x in range(1,6)]\n",
    "    \n",
    "    cnn_mdl = [mdlPath + \"cnn_m\"+ str(x) for x in range(1,6)]\n",
    "    \n",
    "#     for (tf,mfncf) in zip(inFile,modelFile,cnn_mdl):\n",
    "#         data = np.load(tf)\n",
    "#         # 训练模 \n",
    "#         trainModel(data,mf,cf)  \n",
    "    \n",
    "    data = np.load(inFile[0])\n",
    "    test = np.load(testFile[0])\n",
    "    trainModel(data,modelFile[0],cnn_mdl[0],test,textFile[0])\n",
    "    \n",
    "    # 测试模型\n",
    "#     test = np.load(testFile[0])\n",
    "#     loadPred(testFile[0],test,cnn_mdl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
