{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora,models,similarities,utils\n",
    "import logging\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from keras.preprocessing import text,sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLabel(x):\n",
    "    if x == '__label__NEGATIVE':\n",
    "        lable = '0'\n",
    "    elif x== '__label__POSITIVE':\n",
    "        lable = '1'\n",
    "    else:\n",
    "        print \"x=\",x\n",
    "        lable = '0'\n",
    "    return lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取训练数据\n",
    "def getTrainSet(inFile,ptype):\n",
    "    # 训练集\n",
    "    train_set=[]\n",
    "    title_set = []\n",
    "    # 读入训练数据  \n",
    "    f=open(inFile)\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        article = line.replace('\\n','').split(\" \")\n",
    "        if ptype == 'train':\n",
    "            title = getLabel(article[0])\n",
    "        elif ptype== 'test':\n",
    "            title = article[0]\n",
    "        title_set.append(title)\n",
    "        # 内容\n",
    "        train_set.append(article[1:])\n",
    "\n",
    "    f.close()\n",
    "        \n",
    "    return (title_set,train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练word2vec\n",
    "def trainModel(inFile,modelFile,vecFile):\n",
    "    # 读入数据    \n",
    "    title_set,data_set = getTrainSet(inFile,'train')\n",
    "    \n",
    "    # 训练\n",
    "    # 少于min_count次数的单词会被丢弃掉, 默认值为5\n",
    "    # size = 神经网络的隐藏层的单元数 default value is 100\n",
    "    # workers= 控制训练的并行:default = 1 worker (no parallelization) 只有在安装了Cython后才有效\n",
    "    model = models.Word2Vec(data_set,min_count=5,window=10,size = 200,workers=4)\n",
    "    \n",
    "    # 存储模型\n",
    "    model.save(modelFile)\n",
    "    \n",
    "    # 存储vector\n",
    "    model.wv.save_word2vec_format(vecFile, binary=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把原始文本转化为由词汇表索引表示的矩阵\n",
    "def fastBuildSeq(inFile,outFile,modelFile,vecFile,ptype):\n",
    "    # 读入数据\n",
    "    title_set,data_set = getTrainSet(inFile,ptype)\n",
    "    \n",
    "    # 装载模型\n",
    "    model = models.Word2Vec.load(modelFile)\n",
    "    word_vec = model.wv.load_word2vec_format(vecFile, binary=True) \n",
    "    \n",
    "    # 使用dir(object)查看对象的属性\n",
    "    # 对每一个文章做转换      \n",
    "    # 注意：由于word2vec的向量在训练的时候用的是unicode的编码，\n",
    "    # 所以在字典里面匹配key的时候，需要把key转化为unicode的编码，使用decode('utf-8')\n",
    "    transfrom = []\n",
    "    for news in data_set:\n",
    "        trs_news = [word_vec.vocab[w.decode('utf-8')].index for w in news if w.decode('utf-8') in word_vec.vocab]\n",
    "#         # --- 调试\n",
    "#         trs_news = []\n",
    "#         for w in news:\n",
    "#             if w.decode('utf-8') in word_vec.vocab:\n",
    "#                 print \"in vocab = \",w.decode('utf-8')\n",
    "#                 trs_news.append((word_vec.vocab[w.decode('utf-8')].index,w))\n",
    "#         # --\n",
    "        transfrom.append(trs_news)\n",
    "    \n",
    "#     for x in transfrom:\n",
    "#         print x\n",
    "    \n",
    "    # 对文字序列做补齐 ，补齐长度=最长的文章长度 ，补齐在最后，补齐用的词汇默认是词汇表index=0的词汇，也可通过value指定\n",
    "    # 训练好的w2v词表的index = 0 对应的词汇是空格\n",
    "    X = sequence.pad_sequences(transfrom,maxlen=200,padding='post')\n",
    "    y = np.array([int(i) for i in title_set])\n",
    "\n",
    "    # 保存到文件\n",
    "    np.save(outFile,np.column_stack([X,y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data2Mat(inFile,modelFile,vecFile,partOut,totalOut,ptype):\n",
    "    \n",
    "    # 使用训练出的任意一个词向量，把全部train数据转化为向量矩阵\n",
    "    # 把分词以后的文本转化为供CNN训练的数据矩阵\n",
    "    # 由于原始数据较大，每10w分割为一个文件，分别转化\n",
    "    for (tf,po) in zip(inFile,partOut):\n",
    "        fastBuildSeq(tf,po,modelFile,vecFile,ptype)\n",
    "    \n",
    "    # 把转化完成的5个数据矩阵做合并\n",
    "    mergeNpy(partOut,totalOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeNpy(part,total):\n",
    "    # 把转化完成的5个数据矩阵做合并\n",
    "    for idx,f in enumerate(part):\n",
    "        if idx == 0:\n",
    "            tmp = np.load(f)\n",
    "            mat = tmp\n",
    "        else:\n",
    "            tmp = np.load(f)\n",
    "            mat = np.vstack([mat,tmp])\n",
    "       \n",
    "    np.save(total,mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # 定义文件路径\n",
    "    dataPath = \"/home/hadoop/DataSencise/bdci2017/BDCI2017-360/data/\"\n",
    "    mdlPath = \"/home/hadoop/DataSencise/bdci2017/BDCI2017-360/model/\"\n",
    "    \n",
    "    # 训练数据\n",
    "    inFile = [dataPath + \"train/train_m\"+ str(x) + \".txt\" for x in range(1,6)]\n",
    "    modelFile = [mdlPath + \"w2v_m\"+ str(x) + \".mdl\" for x in range(1,6)]\n",
    "    vecFile = [mdlPath + \"w2v_m\"+ str(x) + \".bin\" for x in range(1,6)]\n",
    "    trainPartMat = [dataPath + \"train/train_m\"+ str(x) + \".npy\" for x in range(1,6)]\n",
    "    trainTotalMat = dataPath + \"train/train_totalMat.npy\"\n",
    "    \n",
    "    # 测试数据\n",
    "    testFile = [dataPath + \"test/test_m\"+ str(x) + \".txt\" for x in range(1,6)]\n",
    "    # 定义输出文件名\n",
    "    testPartMat = [dataPath + \"test/test_m\"+ str(x) + \".npy\" for x in range(1,6)]\n",
    "    testTotalMat = dataPath + \"test/test_totalMat.npy\"\n",
    "    \n",
    "    # 训练词向量模型\n",
    "    # 把原始train数据，每10w条为一组，分别训练词向量\n",
    "    # 一共训练出5个词向量模型\n",
    "#     for (tf,mf,vf) in zip(inFile,modelFile,vecFile):\n",
    "#         trainModel(tf,mf,vf)\n",
    "    \n",
    "#     # 把训练数据转成矩阵\n",
    "#     data2Mat(inFile,modelFile[0],vecFile[0],trainPartMat,trainTotalMat,'train')\n",
    "\n",
    "    # 把测试数据转成矩阵\n",
    "    data2Mat(testFile,modelFile[0],vecFile[0],testPartMat,testTotalMat,'test')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'd64d0c87e3c51019b922df146f8b7eab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2436fc2ab63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-f8efcff18cd2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# 把测试数据转成矩阵\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mdata2Mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestFile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodelFile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvecFile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestPartMat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestTotalMat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c794a1c1927f>\u001b[0m in \u001b[0;36mdata2Mat\u001b[0;34m(inFile, modelFile, vecFile, partOut, totalOut, ptype)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 由于原始数据较大，每10w分割为一个文件，分别转化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minFile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpartOut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mfastBuildSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodelFile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvecFile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mptype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 把转化完成的5个数据矩阵做合并\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7e97e4dac851>\u001b[0m in \u001b[0;36mfastBuildSeq\u001b[0;34m(inFile, outFile, modelFile, vecFile, ptype)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# 训练好的w2v词表的index = 0 对应的词汇是空格\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfrom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitle_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# 保存到文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'd64d0c87e3c51019b922df146f8b7eab'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
